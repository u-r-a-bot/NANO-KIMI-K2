{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f226d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:06.719767Z",
     "start_time": "2025-09-07T15:53:02.595420Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"moonshotai/Kimi-K2-Instruct\",trust_remote_code = True)\n",
    "# Vocab size: 163,842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a98dfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed11ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bbba6b13284c8c8edf2910c1872347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee665bd9c114a898963d3fa301d38c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa5573009a546c78adcc2ed00b2a454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26c050d3c3f490190fd1ad42960c92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6e4e81b642442892eaffee81a2940d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
    "# Vocab size: 49,152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a96e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:08.819054Z",
     "start_time": "2025-09-07T15:53:08.811928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19180, 1133, 67]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello workd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22261499",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:09.343839Z",
     "start_time": "2025-09-07T15:53:09.338659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello workd'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([19180, 1133, 67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5025628d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:09.687904Z",
     "start_time": "2025-09-07T15:53:09.684787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163842"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86573866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:10.113634Z",
     "start_time": "2025-09-07T15:53:10.110111Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Union, Literal,Tuple\n",
    "from config import Config\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63c480d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:10.727787Z",
     "start_time": "2025-09-07T15:53:10.722971Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, embed_dim = 768, vocab_size = 163842):\n",
    "        super().__init__()\n",
    "        self.table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.table(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "babfa58b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:49:53.485383Z",
     "start_time": "2025-09-07T15:49:52.111272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = Embeddings()\n",
    "embedder(torch.LongTensor(tokenizer.encode(\"Hello workd\"))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd6cbf35923f6c",
   "metadata": {},
   "source": [
    "# ROPE\n",
    "Applying rope using torch tune, borrowed from torchtune repo as torchtune was not installing for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc353516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:45:28.570871Z",
     "start_time": "2025-09-07T15:45:26.649198Z"
    }
   },
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        max_seq_len: int = 4096,\n",
    "        base: int = 10_000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Embedding dimension.\n",
    "            max_seq_len (int): Maximum expected sequence length.\n",
    "            base (int): The base for the geometric progression.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.rope_init()\n",
    "\n",
    "    def rope_init(self):\n",
    "        theta = 1.0 / (\n",
    "            self.base\n",
    "            ** (torch.arange(0, self.dim, 2)[: (self.dim // 2)].float() / self.dim)\n",
    "        )\n",
    "        self.register_buffer(\"theta\", theta, persistent=False)\n",
    "        self.build_rope_cache(self.max_seq_len)\n",
    "\n",
    "    def build_rope_cache(self, max_seq_len: int = 4096) -> None:\n",
    "        seq_idx = torch.arange(\n",
    "            max_seq_len, dtype=self.theta.dtype, device=self.theta.device\n",
    "        )\n",
    "        idx_theta = torch.einsum(\"i, j -> ij\", seq_idx, self.theta).float()\n",
    "        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "        self.register_buffer(\"cache\", cache, persistent=False)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, *, input_pos: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            input_pos (Optional[torch.Tensor]): Tensor containing position ids of each token.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        rope_cache = (\n",
    "            self.cache[:seq_len] if input_pos is None else self.cache[input_pos]\n",
    "        )\n",
    "        xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "        x_out = torch.stack(\n",
    "            [\n",
    "                xshaped[..., 0] * rope_cache[..., 0]\n",
    "                - xshaped[..., 1] * rope_cache[..., 1],\n",
    "                xshaped[..., 1] * rope_cache[..., 0]\n",
    "                + xshaped[..., 0] * rope_cache[..., 1],\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        x_out = x_out.flatten(3)\n",
    "        return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8681656c2c5f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 1024, 8, 64])\n",
      "Output shape: torch.Size([2, 1024, 8, 64])\n",
      "\n",
      "Single token input shape:  torch.Size([2, 1, 8, 64])\n",
      "Single token output shape: torch.Size([2, 1, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 1024\n",
    "num_heads = 8\n",
    "head_dim = 64\n",
    "\n",
    "# --- Test ---\n",
    "rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=seq_len)\n",
    "input_tensor = torch.randn(batch_size, seq_len, num_heads, head_dim)\n",
    "output_tensor = rope(input_tensor)\n",
    "\n",
    "print(f\"Input shape:  {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output_tensor.shape}\")\n",
    "\n",
    "# --- Test with 'input_pos' for inference-style caching ---\n",
    "# Simulating a single new token at position 5\n",
    "input_pos_tensor = torch.tensor([5], dtype=torch.long)\n",
    "single_token_tensor = torch.randn(batch_size, 1, num_heads, head_dim)\n",
    "output_single_token = rope(single_token_tensor, input_pos=input_pos_tensor)\n",
    "\n",
    "print(f\"\\nSingle token input shape:  {single_token_tensor.shape}\")\n",
    "print(f\"Single token output shape: {output_single_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3aaa6",
   "metadata": {},
   "source": [
    "# Row and Column  Parallelism not needed\n",
    "We dont need Row and Column parallel. (Used for multigpu training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab58e90",
   "metadata": {},
   "source": [
    "# Multi Head Attention Module\n",
    "This will be simplified attention module with skipping Model parallelism, Low rank adaptation.\n",
    "\n",
    "There is a change in use of Forward method because we are using a using different implementation of Rotary Embeddings which uses cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c19b5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Latent Attention (MLA) Layer\n",
    "    Attributes:\n",
    "        dim(int): Dimensionality of input features\n",
    "        n_heads(int): Number of heads\n",
    "        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.\n",
    "        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.\n",
    "        qk_head_dim (int): Total dimensionality of query/key projections.\n",
    "        v_head_dim (int): Dimensionality of value projections.\n",
    "        softmax_scale (float): Scaling factor for softmax in attention computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.n_heads = config.n_heads\n",
    "        self.qk_nope_head_dim = config.qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = config.qk_rope_head_dim\n",
    "        self.qk_head_dim = config.qk_rope_head_dim\n",
    "        self.qk_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n",
    "        self.v_head_dim = config.v_head_dim\n",
    "        # Projections\n",
    "        \n",
    "        self.wq = nn.Linear(self.dim, self.n_heads * self.qk_head_dim, bias=False)\n",
    "        self.wk = nn.Linear(self.dim, self.n_heads * self.qk_head_dim, bias=False)\n",
    "        self.wv = nn.Linear(self.dim, self.n_heads * self.v_head_dim, bias=False)\n",
    "        self.rope = RotaryPositionalEmbeddings(\n",
    "            dim=self.qk_rope_head_dim, \n",
    "            max_seq_len=config.max_seq_length\n",
    "        )\n",
    "        self.register_buffer(\"k_cache\", torch.zeros(config.max_batch_size, config.max_seq_length, config.n_heads, self.qk_head_dim))\n",
    "        self.register_buffer(\"v_cache\", torch.zeros(config.max_batch_size, config.max_seq_length, config.n_heads, config.v_head_dim))\n",
    "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
    "        self.c_v_linear = nn.Linear(in_features=self.n_heads * self.v_head_dim, out_features=self.dim)\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor, start_pos:int, input_pos:Optional[int], mask:Optional[torch.Tensor]):\n",
    "        b, seq_len,_ = x.size()\n",
    "        end_pos = start_pos + seq_len\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.wq(x).view(b, seq_len, self.n_heads, self.qk_head_dim)\n",
    "        k = self.wk(x).view(b, seq_len, self.n_heads, self.qk_head_dim) \n",
    "        v = self.wv(x).view(b, seq_len, self.n_heads, self.v_head_dim)\n",
    "        \n",
    "        q_nope, q_rope = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        k_nope, k_rope = torch.split(k, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        # original used q_pe for rope (erase after implementation)\n",
    "        q_rope = self.rope(q_rope,input_pos = input_pos)\n",
    "        k_rope = self.rope(k_rope, input_pos=input_pos)\n",
    "\n",
    "        # recombone the rotated and not rotated q and k\n",
    "        q = torch.cat([q_nope, q_rope], dim=-1)\n",
    "        k = torch.cat([k_nope, k_rope], dim=-1)\n",
    "        \n",
    "        # Update cache\n",
    "        self.k_cache[b, start_pos:end_pos] = k\n",
    "        self.v_cache[b, start_pos:end_pos] = v\n",
    "\n",
    "        # Trying einstein notation :)\n",
    "        scores = torch.einsum(\"bshd,bthd->bsht\", q, self.k_cache[:b, :end_pos]) * self.softmax_scale\n",
    "        #batch,seq_length_query,num_heads,head_dim   | batch,seq_lenght_key,num_heads,head_dim ->batch,seq_length_key,num_heads,seq_lenght_key\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores += mask.unsqueeze(1)\n",
    "        # Get Attention Weights\n",
    "\n",
    "        weights = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)\n",
    "        \n",
    "        # Context vector\n",
    "        c_v = torch.einsum(\"bsht,bthd->bshd\", weights, self.v_cache[:b, :end_pos])\n",
    "        return self.c_v_linear(c_v.flatten(2))\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b27eea",
   "metadata": {},
   "source": [
    "# MLP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c0133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim:int , inter_dim: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim , inter_dim)\n",
    "        self.w2 = nn.Linear(inter_dim, dim)\n",
    "        self.w3 = nn.Linear(dim , inter_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1678ba2",
   "metadata": {},
   "source": [
    "# Mixture of Experts\n",
    "The expert class has input projected upwards then passed through silu then **element wise multiplied** with another upward projection to get the output.\n",
    "Three modules\n",
    "\n",
    "- Experts\n",
    "- Gate\n",
    "- MoE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4387c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "        Expert Layer for Mixture of Experts(MoE)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim:int, hid_dim:int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, hid_dim)\n",
    "        self.w2 = nn.Linear(hid_dim,dim)\n",
    "        self.w3 = nn.Linear(dim, hid_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self,config:Config):\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.topk = config.n_activated_experts\n",
    "        self.n_groups = config.n_expert_groups\n",
    "        self.score_func = config.score_func\n",
    "        self.router_scale = config.route_scale\n",
    "        self.weight = nn.Parameter((torch.empty(config.n_routed_experts, config.dim)))\n",
    "        self.bias = nn.Parameter(torch.empty(config.n_routed_experts, dtype=torch.float32)) if self.dim == 7168 else None\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> Tuple[torch.Tensor,torch.Tensor]:\n",
    "        \"\"\"\n",
    "            Forward Pass for gating mechanism\n",
    "            Args:\n",
    "                x (torch.Tensor): Input Tensor\n",
    "            Returns:\n",
    "                Tuple[torch.Tensor, torch.Tensor]: Routing weights\n",
    "        \"\"\"\n",
    "        scores = nn.Linear(x, self.weight)\n",
    "        if self.score_func == \"softmax\":\n",
    "            scores = scores.softmax(dim=-1, dtype=torch.float32)\n",
    "        else:\n",
    "            scores = scores.sigmoid()\n",
    "        original_scores = scores\n",
    "        if self.bias is not None:\n",
    "            scores = scores+ self.bias\n",
    "        if self.n_groups > 1:\n",
    "            scores = scores.view(x.size(0), self.n_groups, -1)\n",
    "            if self.bias is None:\n",
    "                group_scores = scores.amax(dim=-1)\n",
    "            else:\n",
    "                group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n",
    "            indices =group_scores.topk(self.topk_groups,dim = -1)[1]\n",
    "            mask = scores.new_ones(x.size(0), self.n_groups, dtype=bool).scatter_(1, indices, False)\n",
    "            scores = scores.masked_fill_(mask.unsqueeze(-1), float(\"-inf\")).flatten(1)\n",
    "        indices = torch.topk(scores,self.topk,dim = -1)[1]\n",
    "        weights = original_scores.gather(1,indices)\n",
    "        if self.score_func == \"sigmoid\":\n",
    "            weights /= weights.sum(dim=-1, keepdim=True)\n",
    "        weights *= self.route_scale\n",
    "        return weights.type_as(x), indices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.dim  =  config.dim\n",
    "        self.n_routed_experts = config.n_routed_experts\n",
    "        self.n_local_experts = config.n_routed_experts \n",
    "        self.n_activated_experts = config.n_activated_experts\n",
    "        self.experts_start_idx = self.n_local_experts\n",
    "        self.experts_end_idx = self.experts_start_idx + self.n_local_experts\n",
    "        self.gate = Gate(config)\n",
    "        self.experts = nn.ModuleList([Expert(config.dim, config.moe_inter_dim) if self.experts_start_idx <= i < self.experts_end_idx else None\n",
    "                                      for i in range(self.n_routed_experts)])\n",
    "        self.shared_experts = MLP(config.dim, config.n_shared_experts * config.moe_inter_dim)\n",
    "\n",
    "    def forward(self,x: torch.Tensor):\n",
    "        shape = x.size()\n",
    "        x = x.view(-1, self.dim)\n",
    "        weights, indices = self.gate(x)\n",
    "        y = torch.zeros_like(x)\n",
    "        counts = torch.bincount(indices.flatten(), minlength=self.n_routed_experts).tolist()\n",
    "        for i in range(self.experts_start_idx, self.experts_end_idx):\n",
    "            if counts[i] == 0:\n",
    "                continue\n",
    "            expert = self.experts[i]\n",
    "            idx, top = torch.where(indices == i)\n",
    "            y[idx] += expert(x[idx]) * weights[idx, top, None]\n",
    "        z = self.shared_experts(x)\n",
    "        return (y + z).view(shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2807b28",
   "metadata": {},
   "source": [
    "# Transformer and Block\n",
    "- Block Module\n",
    "- Transformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e219026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,layer_id: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.attn = MLA(config)\n",
    "        self.ffn = MLP(config.dim, config.inter_dim) if layer_id < config.n_dense_layers else MoE(config)\n",
    "        self.attn_norm = nn.RMSNorm(config.dim)\n",
    "        self.ffn_norm = nn.RMSNorm(config.dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos:int, freqs_cis:torch.Tensor, mask: Optional[torch.Tensor])-> torch.Tensor:\n",
    "        x = x+ self.attn(self.attn_norm(x),start_pos, freqs_cis,mask )\n",
    "        x = x+ self.ffn(self.ffn_norm(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.embed = Embeddings(config.dim, config.vocab_size)\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(config.n_layers):\n",
    "            self.layers.append(Block(layer_id, config))\n",
    "        self.norm = nn.RMSNorm(config.dim)\n",
    "        self.head = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int = 0):\n",
    "        seqlen = tokens.size(1)\n",
    "        h = self.embed(tokens)\n",
    "        \n",
    "        # Create input_pos tensor for RoPE\n",
    "        input_pos = torch.arange(start_pos, start_pos + seqlen, device=tokens.device)\n",
    "        \n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device).triu_(1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, input_pos, mask)\n",
    "        \n",
    "        h = self.norm(h)[:, -1]\n",
    "        logits = self.head(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd259fde",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "- Preparing Data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a52847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
