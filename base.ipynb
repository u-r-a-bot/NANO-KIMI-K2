{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f226d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:06.719767Z",
     "start_time": "2025-09-07T15:53:02.595420Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"moonshotai/Kimi-K2-Instruct\",trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04a96e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:08.819054Z",
     "start_time": "2025-09-07T15:53:08.811928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19180, 1133, 67]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello workd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22261499",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:09.343839Z",
     "start_time": "2025-09-07T15:53:09.338659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello workd'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([19180, 1133, 67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5025628d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:09.687904Z",
     "start_time": "2025-09-07T15:53:09.684787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163842"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86573866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:10.113634Z",
     "start_time": "2025-09-07T15:53:10.110111Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Union\n",
    "from config import Config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d63c480d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:10.727787Z",
     "start_time": "2025-09-07T15:53:10.722971Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, embed_dim = 768, vocab_size = 163842):\n",
    "        super().__init__()\n",
    "        self.table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.table(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "babfa58b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:49:53.485383Z",
     "start_time": "2025-09-07T15:49:52.111272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = Embeddings()\n",
    "embedder(torch.LongTensor(tokenizer.encode(\"Hello workd\"))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd6cbf35923f6c",
   "metadata": {},
   "source": [
    "# ROPE\n",
    "Applying rope using torch tune, borrowed from torchtune repo as torchtune was not installing for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc353516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:45:28.570871Z",
     "start_time": "2025-09-07T15:45:26.649198Z"
    }
   },
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        max_seq_len: int = 4096,\n",
    "        base: int = 10_000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Embedding dimension.\n",
    "            max_seq_len (int): Maximum expected sequence length.\n",
    "            base (int): The base for the geometric progression.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.rope_init()\n",
    "\n",
    "    def rope_init(self):\n",
    "        theta = 1.0 / (\n",
    "            self.base\n",
    "            ** (torch.arange(0, self.dim, 2)[: (self.dim // 2)].float() / self.dim)\n",
    "        )\n",
    "        self.register_buffer(\"theta\", theta, persistent=False)\n",
    "        self.build_rope_cache(self.max_seq_len)\n",
    "\n",
    "    def build_rope_cache(self, max_seq_len: int = 4096) -> None:\n",
    "        seq_idx = torch.arange(\n",
    "            max_seq_len, dtype=self.theta.dtype, device=self.theta.device\n",
    "        )\n",
    "        idx_theta = torch.einsum(\"i, j -> ij\", seq_idx, self.theta).float()\n",
    "        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "        self.register_buffer(\"cache\", cache, persistent=False)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, *, input_pos: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            input_pos (Optional[torch.Tensor]): Tensor containing position ids of each token.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        rope_cache = (\n",
    "            self.cache[:seq_len] if input_pos is None else self.cache[input_pos]\n",
    "        )\n",
    "        xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "        x_out = torch.stack(\n",
    "            [\n",
    "                xshaped[..., 0] * rope_cache[..., 0]\n",
    "                - xshaped[..., 1] * rope_cache[..., 1],\n",
    "                xshaped[..., 1] * rope_cache[..., 0]\n",
    "                + xshaped[..., 0] * rope_cache[..., 1],\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        x_out = x_out.flatten(3)\n",
    "        return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8681656c2c5f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 1024, 8, 64])\n",
      "Output shape: torch.Size([2, 1024, 8, 64])\n",
      "\n",
      "Single token input shape:  torch.Size([2, 1, 8, 64])\n",
      "Single token output shape: torch.Size([2, 1, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 1024\n",
    "num_heads = 8\n",
    "head_dim = 64\n",
    "\n",
    "# --- Test ---\n",
    "rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=seq_len)\n",
    "input_tensor = torch.randn(batch_size, seq_len, num_heads, head_dim)\n",
    "output_tensor = rope(input_tensor)\n",
    "\n",
    "print(f\"Input shape:  {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output_tensor.shape}\")\n",
    "\n",
    "# --- Test with 'input_pos' for inference-style caching ---\n",
    "# Simulating a single new token at position 5\n",
    "input_pos_tensor = torch.tensor([5], dtype=torch.long)\n",
    "single_token_tensor = torch.randn(batch_size, 1, num_heads, head_dim)\n",
    "output_single_token = rope(single_token_tensor, input_pos=input_pos_tensor)\n",
    "\n",
    "print(f\"\\nSingle token input shape:  {single_token_tensor.shape}\")\n",
    "print(f\"Single token output shape: {output_single_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3aaa6",
   "metadata": {},
   "source": [
    "# Row and Column  Parallelism not needed\n",
    "We dont need Row and Column parallel. (Used for multigpu training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab58e90",
   "metadata": {},
   "source": [
    "# Multi Head Attention Module\n",
    "This will be simplified attention module with skipping Model parallelism, Low rank adaptation.\n",
    "\n",
    "There is a change in use of Forward method because we are using a using different implementation of Rotary Embeddings which uses cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b5f79",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3450621908.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self, config: Config):\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class MLA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Latent Attention (MLA) Layer\n",
    "    Attributes:\n",
    "        dim(int): Dimensionality of input features\n",
    "        n_heads(int): Number of heads\n",
    "        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.\n",
    "        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.\n",
    "        qk_head_dim (int): Total dimensionality of query/key projections.\n",
    "        v_head_dim (int): Dimensionality of value projections.\n",
    "        softmax_scale (float): Scaling factor for softmax in attention computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.n_heads = config.n_heads\n",
    "        self.qk_nope_head_dim = config.qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = config.qk_rope_head_dim\n",
    "        self.qk_head_dim = config.qk_rope_head_dim\n",
    "        self.qk_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n",
    "        self.v_head_dim = config.v_head_dim\n",
    "        # Projections\n",
    "        \n",
    "        self.wq = nn.Linear(self.dim, self.n_heads * self.qk_head_dim, bias=False)\n",
    "        self.wk = nn.Linear(self.dim, self.n_heads * self.qk_head_dim, bias=False)\n",
    "        self.wv = nn.Linear(self.dim, self.n_heads * self.v_head_dim, bias=False)\n",
    "        self.rope = RotaryPositionalEmbeddings(\n",
    "            dim=self.qk_rope_head_dim, \n",
    "            max_seq_len=config.max_seq_length\n",
    "        )\n",
    "        self.register_buffer(\"k_cache\", torch.zeros(config.max_batch_size, config.max_seq_length, config.n_heads, self.qk_head_dim))\n",
    "        self.register_buffer(\"v_cache\", torch.zeros(config.max_batch_size, config.max_seq_length, config.n_heads, config.v_head_dim))\n",
    "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
    "        self.c_v_linear = nn.Linear(in_features=self.n_heads * self.v_head_dim, out_features=self.dim)\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor, start_pos:int, input_pos:Optional[int], mask:Optional[torch.Tensor]):\n",
    "        b, seq_len,_ = x.size()\n",
    "        end_pos = start_pos + seq_len\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.wq(x).view(b, seq_len, self.n_heads, self.qk_head_dim)\n",
    "        k = self.wk(x).view(b, seq_len, self.n_heads, self.qk_head_dim) \n",
    "        v = self.wv(x).view(b, seq_len, self.n_heads, self.v_head_dim)\n",
    "        \n",
    "        q_nope, q_rope = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        k_nope, k_rope = torch.split(k, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        # original used q_pe for rope (erase after implementation)\n",
    "        q_rope = self.rope(q_rope,input_pos = input_pos)\n",
    "        k_rope = self.rope(k_rope, input_pos=input_pos)\n",
    "\n",
    "        # recombone the rotated and not rotated q and k\n",
    "        q = torch.cat([q_nope, q_rope], dim=-1)\n",
    "        k = torch.cat([k_nope, k_rope], dim=-1)\n",
    "        \n",
    "        # Update cache\n",
    "        self.k_cache[b, start_pos:end_pos] = k\n",
    "        self.v_cache[b, start_pos:end_pos] = v\n",
    "\n",
    "        # Trying einstein notation :)\n",
    "        scores = torch.einsum(\"bshd,bthd->bsht\", q, self.k_cache[:b, :end_pos]) * self.softmax_scale\n",
    "        #batch,seq_length_query,num_heads,head_dim   | batch,seq_lenght_key,num_heads,head_dim ->batch,seq_length_key,num_heads,seq_lenght_key\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores += mask.unsqueeze(1)\n",
    "        # Get Attention Weights\n",
    "\n",
    "        weights = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)\n",
    "        \n",
    "        # Context vector\n",
    "        c_v = torch.einsum(\"bsht,bthd->bshd\", weights, self.v_cache[:b, :end_pos])\n",
    "        return self.c_v_linear(c_v.flatten(2))\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387c0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
