{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f226d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:06.719767Z",
     "start_time": "2025-09-07T15:53:02.595420Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"moonshotai/Kimi-K2-Instruct\",trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a96e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:08.819054Z",
     "start_time": "2025-09-07T15:53:08.811928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19180, 1133, 67]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello workd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22261499",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:09.343839Z",
     "start_time": "2025-09-07T15:53:09.338659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello workd'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([19180, 1133, 67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5025628d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:09.687904Z",
     "start_time": "2025-09-07T15:53:09.684787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163842"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86573866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:10.113634Z",
     "start_time": "2025-09-07T15:53:10.110111Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d63c480d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:53:10.727787Z",
     "start_time": "2025-09-07T15:53:10.722971Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, embed_dim = 768, vocab_size = 163842):\n",
    "        super().__init__()\n",
    "        self.table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.table(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "babfa58b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:49:53.485383Z",
     "start_time": "2025-09-07T15:49:52.111272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = Embeddings()\n",
    "embedder(torch.LongTensor(tokenizer.encode(\"Hello workd\"))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd6cbf35923f6c",
   "metadata": {},
   "source": [
    "# ROPE\n",
    "Applying rope using torch tune, borrowed from torchtune repo as torchtune was not installing for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc353516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:45:28.570871Z",
     "start_time": "2025-09-07T15:45:26.649198Z"
    }
   },
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        max_seq_len: int = 4096,\n",
    "        base: int = 10_000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Embedding dimension.\n",
    "            max_seq_len (int): Maximum expected sequence length.\n",
    "            base (int): The base for the geometric progression.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.rope_init()\n",
    "\n",
    "    def rope_init(self):\n",
    "        theta = 1.0 / (\n",
    "            self.base\n",
    "            ** (torch.arange(0, self.dim, 2)[: (self.dim // 2)].float() / self.dim)\n",
    "        )\n",
    "        self.register_buffer(\"theta\", theta, persistent=False)\n",
    "        self.build_rope_cache(self.max_seq_len)\n",
    "\n",
    "    def build_rope_cache(self, max_seq_len: int = 4096) -> None:\n",
    "        seq_idx = torch.arange(\n",
    "            max_seq_len, dtype=self.theta.dtype, device=self.theta.device\n",
    "        )\n",
    "        idx_theta = torch.einsum(\"i, j -> ij\", seq_idx, self.theta).float()\n",
    "        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "        self.register_buffer(\"cache\", cache, persistent=False)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, *, input_pos: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            input_pos (Optional[torch.Tensor]): Tensor containing position ids of each token.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        rope_cache = (\n",
    "            self.cache[:seq_len] if input_pos is None else self.cache[input_pos]\n",
    "        )\n",
    "        xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "        x_out = torch.stack(\n",
    "            [\n",
    "                xshaped[..., 0] * rope_cache[..., 0]\n",
    "                - xshaped[..., 1] * rope_cache[..., 1],\n",
    "                xshaped[..., 1] * rope_cache[..., 0]\n",
    "                + xshaped[..., 0] * rope_cache[..., 1],\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        x_out = x_out.flatten(3)\n",
    "        return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8681656c2c5f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 1024, 8, 64])\n",
      "Output shape: torch.Size([2, 1024, 8, 64])\n",
      "\n",
      "Single token input shape:  torch.Size([2, 1, 8, 64])\n",
      "Single token output shape: torch.Size([2, 1, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 1024\n",
    "num_heads = 8\n",
    "head_dim = 64\n",
    "\n",
    "# --- Test ---\n",
    "rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=seq_len)\n",
    "input_tensor = torch.randn(batch_size, seq_len, num_heads, head_dim)\n",
    "output_tensor = rope(input_tensor)\n",
    "\n",
    "print(f\"Input shape:  {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output_tensor.shape}\")\n",
    "\n",
    "# --- Test with 'input_pos' for inference-style caching ---\n",
    "# Simulating a single new token at position 5\n",
    "input_pos_tensor = torch.tensor([5], dtype=torch.long)\n",
    "single_token_tensor = torch.randn(batch_size, 1, num_heads, head_dim)\n",
    "output_single_token = rope(single_token_tensor, input_pos=input_pos_tensor)\n",
    "\n",
    "print(f\"\\nSingle token input shape:  {single_token_tensor.shape}\")\n",
    "print(f\"Single token output shape: {output_single_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3aaa6",
   "metadata": {},
   "source": [
    "# Row and Column  Parallelism not needed\n",
    "We dont need Row and Column parallel. (Used for multigpu training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab58e90",
   "metadata": {},
   "source": [
    "# Multi Head Attention Module\n",
    "This will be simplified attention module with skipping Model parallelism, Low rank adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b5f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
